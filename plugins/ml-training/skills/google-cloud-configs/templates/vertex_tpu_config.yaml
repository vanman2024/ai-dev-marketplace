# Vertex AI TPU Training Job Configuration Templates
# TPUs are optimized for TensorFlow and JAX workloads

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 1: TPU v3-8 (Standard TPU)
# Use for: Most TensorFlow/JAX training
# Cost: ~$8/hour
# Specs: 8 cores, 128GB HBM
# ═══════════════════════════════════════════════════════════════════════════

tpu_v3_8:
  display_name: "training-job-tpu-v3-8"
  worker_pool_specs:
    - machine_spec:
        machine_type: "cloud-tpu"
        accelerator_type: "TPU_V3"
        accelerator_count: 8  # TPU v3-8 has 8 cores
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 300
      container_spec:
        image_uri: "gcr.io/your_project/tpu-training:latest"
        command: ["python", "train.py"]
        args:
          - "--tpu_name=local"
          - "--data_path=gs://your_bucket/data"
          - "--model_dir=gs://your_bucket/models"
          - "--batch_size=1024"  # TPUs work best with large batch sizes
        env:
          - name: "TPU_NAME"
            value: "local"
          - name: "XLA_FLAGS"
            value: "--xla_gpu_cuda_data_dir=/usr/local/cuda"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"
  scheduling:
    timeout: "86400s"

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 2: TPU v4-8 (Latest Generation)
# Use for: Cutting-edge performance
# Cost: ~$11/hour
# Specs: 8 cores, 256GB HBM
# ═══════════════════════════════════════════════════════════════════════════

tpu_v4_8:
  display_name: "training-job-tpu-v4-8"
  worker_pool_specs:
    - machine_spec:
        machine_type: "cloud-tpu"
        accelerator_type: "TPU_V4"
        accelerator_count: 8
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 500
      python_package_spec:
        executor_image_uri: "us-docker.pkg.dev/vertex-ai/training/tf-tpu.2-12.py310:latest"
        package_uris: ["gs://your_bucket/trainer.tar.gz"]
        python_module: "trainer.task"
        args:
          - "--use_tpu=true"
          - "--tpu_topology=2x2x2"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 3: TPU v5e-8 (Cost-Optimized)
# Use for: Budget-friendly TPU training
# Cost: ~$2.50/hour
# Specs: 8 cores, optimized for efficiency
# ═══════════════════════════════════════════════════════════════════════════

tpu_v5e_8:
  display_name: "training-job-tpu-v5e-8"
  worker_pool_specs:
    - machine_spec:
        machine_type: "cloud-tpu"
        accelerator_type: "TPU_V5_LITEPOD"
        accelerator_count: 8
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-balanced"
        boot_disk_size_gb: 200
      container_spec:
        image_uri: "gcr.io/your_project/tpu-training:latest"
        command: ["python", "train.py"]
        args:
          - "--tpu=local"
          - "--use_mixed_precision=true"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 4: TPU v3-32 (TPU Pod Slice)
# Use for: Large-scale distributed training
# Cost: ~$32/hour
# Specs: 32 cores (4x v3-8), 512GB HBM
# ═══════════════════════════════════════════════════════════════════════════

tpu_v3_32_pod:
  display_name: "training-job-tpu-v3-32-pod"
  worker_pool_specs:
    - machine_spec:
        machine_type: "cloud-tpu"
        accelerator_type: "TPU_V3"
        accelerator_count: 32
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 500
      container_spec:
        image_uri: "gcr.io/your_project/tpu-training:latest"
        command: ["python", "train.py"]
        args:
          - "--tpu_topology=4x4x2"
          - "--global_batch_size=4096"  # Large batch for pod
          - "--distribution_strategy=tpu"
        env:
          - name: "TPU_TOPOLOGY"
            value: "4x4x2"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"
  scheduling:
    timeout: "259200s"  # 72 hours

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 5: TPU v4-128 (Large TPU Pod)
# Use for: Massive distributed training (GPT-scale)
# Cost: ~$176/hour
# Specs: 128 cores, 4TB HBM
# ═══════════════════════════════════════════════════════════════════════════

tpu_v4_128_pod:
  display_name: "training-job-tpu-v4-128-pod"
  worker_pool_specs:
    - machine_spec:
        machine_type: "cloud-tpu"
        accelerator_type: "TPU_V4"
        accelerator_count: 128
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 1000
      container_spec:
        image_uri: "gcr.io/your_project/tpu-training:latest"
        command: ["python", "train.py"]
        args:
          - "--tpu_topology=8x8x2"
          - "--global_batch_size=16384"
          - "--model_parallel=true"
          - "--pipeline_parallel=true"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"
  scheduling:
    timeout: "604800s"  # 7 days

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 6: Preemptible TPU v3-8
# Use for: Cost-optimized training with checkpointing
# Cost: ~$3.20/hour (60% discount)
# ═══════════════════════════════════════════════════════════════════════════

preemptible_tpu_v3_8:
  display_name: "training-job-tpu-v3-8-preemptible"
  worker_pool_specs:
    - machine_spec:
        machine_type: "cloud-tpu"
        accelerator_type: "TPU_V3"
        accelerator_count: 8
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-balanced"
        boot_disk_size_gb: 200
      container_spec:
        image_uri: "gcr.io/your_project/tpu-training:latest"
        command: ["python", "train.py"]
        args:
          - "--checkpoint_every_n_steps=1000"
          - "--resume_from_latest_checkpoint=true"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"
  scheduling:
    timeout: "172800s"
    restart_job_on_worker_restart: true

# ═══════════════════════════════════════════════════════════════════════════
# TPU-Specific Environment Variables
# ═══════════════════════════════════════════════════════════════════════════

tpu_env_vars:
  # TPU Configuration
  - name: "TPU_NAME"
    value: "local"
  - name: "TPU_TOPOLOGY"
    value: "2x2x2"  # Adjust based on TPU size

  # XLA Optimization Flags
  - name: "XLA_FLAGS"
    value: "--xla_gpu_cuda_data_dir=/usr/local/cuda --xla_tpu_enable_data_parallel_all_reduce_opt=true"

  # JAX Configuration (for JAX workloads)
  - name: "JAX_PLATFORMS"
    value: "tpu"
  - name: "JAX_BACKEND_TARGET"
    value: "local"

  # TensorFlow Configuration
  - name: "TF_CPP_MIN_LOG_LEVEL"
    value: "0"  # Show all logs
  - name: "TF_XLA_FLAGS"
    value: "--tf_xla_auto_jit=2"

  # Performance Settings
  - name: "TPU_MIN_LOG_LEVEL"
    value: "0"
  - name: "GRPC_VERBOSITY"
    value: "ERROR"

# ═══════════════════════════════════════════════════════════════════════════
# Usage Notes
# ═══════════════════════════════════════════════════════════════════════════

usage_notes: |
  TPU Best Practices:

  1. TPU Selection Guide:
     - v5e-8: Development, cost-sensitive training ($2.50/hr)
     - v3-8: Standard training, good balance ($8/hr)
     - v4-8: Latest generation, best performance ($11/hr)
     - v3-32: Large models, distributed training ($32/hr)
     - v4-128: GPT-scale training ($176/hr)

  2. Batch Size Considerations:
     - TPUs work best with LARGE batch sizes (1024-16384)
     - Use global_batch_size for pods
     - Enable gradient accumulation if needed
     - Small batches waste TPU compute

  3. Data Pipeline:
     - Use tf.data or JAX data loaders
     - Prefetch data to TPU memory
     - Store data in GCS buckets in same region
     - Use TFRecord or sharded datasets

  4. Code Requirements:
     - TensorFlow: Use tf.distribute.TPUStrategy
     - JAX: Use jax.pmap for multi-device
     - PyTorch: Use torch_xla (experimental)
     - Avoid control flow (if/while) inside XLA

  5. TPU Topologies:
     - v3-8: 2x2x2 (single device)
     - v3-32: 4x4x2 (4 devices)
     - v4-8: 2x2x2 (single device)
     - v4-128: 8x8x2 (16 devices)

  6. Cost Optimization:
     - Use preemptible TPUs (60% discount)
     - Use v5e for development
     - Clean up unused TPUs immediately
     - Use lifecycle policies for GCS
     - Monitor with Cloud Monitoring

  7. TensorFlow Example:
     ```python
     import tensorflow as tf

     resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='local')
     tf.config.experimental_connect_to_cluster(resolver)
     tf.tpu.experimental.initialize_tpu_system(resolver)
     strategy = tf.distribute.TPUStrategy(resolver)

     with strategy.scope():
         model = create_model()
         model.compile(...)
         model.fit(dataset, epochs=10)
     ```

  8. JAX Example:
     ```python
     import jax
     import jax.numpy as jnp

     # Auto-detects TPU
     devices = jax.devices()
     print(f"Training on {len(devices)} TPU cores")

     # Parallel training with pmap
     @jax.pmap
     def train_step(state, batch):
         ...
     ```

  9. Common Issues:
     - "TPU not found": Check TPU_NAME env var
     - "Compilation timeout": Reduce model complexity
     - "OOM": Reduce batch size or use gradient checkpointing
     - "Slow training": Check data pipeline, increase batch size

  10. Monitoring:
      - Cloud Console: View TPU utilization
      - TensorBoard: Track training metrics
      - Cloud Logging: Debug TPU issues
      - Profiler: Identify bottlenecks

  11. Submitting Jobs:
      gcloud ai custom-jobs create \
        --region=us-central1 \
        --config=this_file.yaml \
        --display-name=my-tpu-job

  12. TPU vs GPU Decision:
      Use TPU when:
      - Training with TensorFlow or JAX
      - Can use large batch sizes (>1024)
      - Matrix operations dominate
      - Budget for initial XLA compilation

      Use GPU when:
      - Using PyTorch primarily
      - Need small batch sizes
      - Have custom CUDA kernels
      - Frequent model changes (avoid recompilation)
