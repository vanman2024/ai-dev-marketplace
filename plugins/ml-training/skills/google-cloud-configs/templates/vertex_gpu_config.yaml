# Vertex AI GPU Training Job Configuration Templates
# Use these presets for different training scenarios

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 1: Single T4 GPU (Budget-Friendly)
# Use for: Small models, prototyping, inference testing
# Cost: ~$0.70/hour
# ═══════════════════════════════════════════════════════════════════════════

single_t4:
  display_name: "training-job-t4"
  worker_pool_specs:
    - machine_spec:
        machine_type: "n1-standard-4"
        accelerator_type: "NVIDIA_TESLA_T4"
        accelerator_count: 1
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 100
      python_package_spec:
        executor_image_uri: "us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13.py310:latest"
        package_uris: ["gs://your_bucket/trainer.tar.gz"]
        python_module: "trainer.task"
        args: ["--epochs=10", "--batch-size=32"]
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 2: Single A100 GPU (Standard Training)
# Use for: Most deep learning training jobs
# Cost: ~$4.00/hour
# ═══════════════════════════════════════════════════════════════════════════

single_a100:
  display_name: "training-job-a100"
  worker_pool_specs:
    - machine_spec:
        machine_type: "a2-highgpu-1g"  # 12 vCPU, 85GB RAM, 1x A100 40GB
        accelerator_type: "NVIDIA_TESLA_A100"
        accelerator_count: 1
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 200
      container_spec:
        image_uri: "gcr.io/your_project/training:latest"
        command: ["python", "train.py"]
        args:
          - "--data_path=gs://your_bucket/data"
          - "--model_dir=gs://your_bucket/models"
          - "--epochs=50"
          - "--batch_size=64"
        env:
          - name: "NCCL_DEBUG"
            value: "INFO"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"
  scheduling:
    timeout: "86400s"  # 24 hours
    restart_job_on_worker_restart: true

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 3: Single A100 80GB (Large Models)
# Use for: Large language models, large vision models
# Cost: ~$5.30/hour
# ═══════════════════════════════════════════════════════════════════════════

single_a100_80gb:
  display_name: "training-job-a100-80gb"
  worker_pool_specs:
    - machine_spec:
        machine_type: "a2-ultragpu-1g"  # 12 vCPU, 170GB RAM, 1x A100 80GB
        accelerator_type: "NVIDIA_A100_80GB"
        accelerator_count: 1
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 500
      container_spec:
        image_uri: "gcr.io/your_project/training:latest"
        command: ["python", "train.py"]
        args:
          - "--model_size=large"
          - "--mixed_precision=bf16"
          - "--gradient_checkpointing=true"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 4: Multi-GPU (4x A100) - Distributed Training
# Use for: Large-scale training, distributed workloads
# Cost: ~$16/hour
# ═══════════════════════════════════════════════════════════════════════════

multi_gpu_4xa100:
  display_name: "training-job-4xa100"
  worker_pool_specs:
    - machine_spec:
        machine_type: "a2-highgpu-4g"  # 48 vCPU, 340GB RAM, 4x A100 40GB
        accelerator_type: "NVIDIA_TESLA_A100"
        accelerator_count: 4
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 500
      container_spec:
        image_uri: "gcr.io/your_project/training:latest"
        command: ["python", "-m", "torch.distributed.launch"]
        args:
          - "--nproc_per_node=4"
          - "--nnodes=1"
          - "--node_rank=0"
          - "train.py"
          - "--distributed=true"
          - "--backend=nccl"
        env:
          - name: "NCCL_DEBUG"
            value: "INFO"
          - name: "NCCL_IB_DISABLE"
            value: "1"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"
  scheduling:
    timeout: "172800s"  # 48 hours

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 5: Multi-Node (2 nodes x 4 GPUs = 8x A100)
# Use for: Very large-scale distributed training
# Cost: ~$32/hour
# ═══════════════════════════════════════════════════════════════════════════

multi_node_8xa100:
  display_name: "training-job-8xa100-multinode"
  worker_pool_specs:
    # Master node
    - machine_spec:
        machine_type: "a2-highgpu-4g"
        accelerator_type: "NVIDIA_TESLA_A100"
        accelerator_count: 4
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 500
      container_spec:
        image_uri: "gcr.io/your_project/training:latest"
        command: ["python", "-m", "torch.distributed.launch"]
        args:
          - "--nproc_per_node=4"
          - "--nnodes=2"
          - "--node_rank=0"
          - "--master_addr=worker-0"
          - "--master_port=23456"
          - "train.py"
    # Worker nodes
    - machine_spec:
        machine_type: "a2-highgpu-4g"
        accelerator_type: "NVIDIA_TESLA_A100"
        accelerator_count: 4
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 500
      container_spec:
        image_uri: "gcr.io/your_project/training:latest"
        command: ["python", "-m", "torch.distributed.launch"]
        args:
          - "--nproc_per_node=4"
          - "--nnodes=2"
          - "--node_rank=1"
          - "--master_addr=worker-0"
          - "--master_port=23456"
          - "train.py"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 6: Preemptible Training (Cost-Optimized)
# Use for: Long training jobs that can handle interruptions
# Cost: ~60% cheaper than standard
# ═══════════════════════════════════════════════════════════════════════════

preemptible_a100:
  display_name: "training-job-preemptible"
  worker_pool_specs:
    - machine_spec:
        machine_type: "a2-highgpu-1g"
        accelerator_type: "NVIDIA_TESLA_A100"
        accelerator_count: 1
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-ssd"
        boot_disk_size_gb: 200
      container_spec:
        image_uri: "gcr.io/your_project/training:latest"
        command: ["python", "train.py"]
        args:
          - "--checkpoint_every_n_steps=500"  # Frequent checkpointing
          - "--resume_from_checkpoint=true"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"
  scheduling:
    timeout: "172800s"
    restart_job_on_worker_restart: true
  service_account: "training-sa@your_project.iam.gserviceaccount.com"
  enable_web_access: false
  enable_dashboard_access: true
  labels:
    cost_center: "ml_training"
    preemptible: "true"

# ═══════════════════════════════════════════════════════════════════════════
# PRESET 7: L4 GPU (Cost-Effective Modern GPU)
# Use for: Mid-size models, good performance/cost ratio
# Cost: ~$1.00/hour
# ═══════════════════════════════════════════════════════════════════════════

single_l4:
  display_name: "training-job-l4"
  worker_pool_specs:
    - machine_spec:
        machine_type: "g2-standard-4"  # 4 vCPU, 16GB RAM
        accelerator_type: "NVIDIA_L4"
        accelerator_count: 1
      replica_count: 1
      disk_spec:
        boot_disk_type: "pd-balanced"
        boot_disk_size_gb: 100
      python_package_spec:
        executor_image_uri: "us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-13.py310:latest"
        package_uris: ["gs://your_bucket/trainer.tar.gz"]
        python_module: "trainer.task"
  base_output_directory:
    output_uri_prefix: "gs://your_bucket/output"

# ═══════════════════════════════════════════════════════════════════════════
# Environment Variables (Common Across All Presets)
# ═══════════════════════════════════════════════════════════════════════════

common_env_vars:
  # CUDA/GPU Configuration
  - name: "CUDA_VISIBLE_DEVICES"
    value: "0"  # Adjust based on GPU count

  # PyTorch Settings
  - name: "TORCH_DISTRIBUTED_DEBUG"
    value: "DETAIL"
  - name: "NCCL_DEBUG"
    value: "INFO"

  # Performance Optimization
  - name: "OMP_NUM_THREADS"
    value: "8"
  - name: "NCCL_IB_DISABLE"
    value: "1"

  # Mixed Precision
  - name: "NVIDIA_TF32_OVERRIDE"
    value: "1"

  # GCS Configuration
  - name: "GCS_READ_CACHE_BLOCK_SIZE_MB"
    value: "64"

# ═══════════════════════════════════════════════════════════════════════════
# Usage Notes
# ═══════════════════════════════════════════════════════════════════════════

usage_notes: |
  1. Choose the right preset for your workload:
     - single_t4: Prototyping, small models
     - single_a100: Standard deep learning
     - single_a100_80gb: Large models (GPT, large ViT)
     - multi_gpu_4xa100: Distributed training
     - multi_node_8xa100: Very large scale
     - preemptible_a100: Cost-optimized long training
     - single_l4: Modern cost-effective option

  2. Machine types:
     - n1-standard: General purpose
     - a2-highgpu: Optimized for A100 GPUs
     - a2-ultragpu: For A100 80GB
     - g2-standard: For L4 GPUs

  3. Disk configuration:
     - pd-ssd: Fast, more expensive
     - pd-balanced: Good balance
     - pd-standard: Cheaper, slower

  4. Container vs Python Package:
     - container_spec: Use custom Docker image
     - python_package_spec: Use pre-built Vertex AI images

  5. Cost optimization:
     - Use preemptible instances
     - Enable checkpointing
     - Use pd-balanced disks
     - Clean up after training

  6. Distributed training:
     - Single machine multi-GPU: Use torch.distributed.launch
     - Multi-node: Configure master_addr and node_rank
     - Enable NCCL debugging for troubleshooting

  7. Submitting jobs:
     gcloud ai custom-jobs create \
       --region=us-central1 \
       --config=this_file.yaml
