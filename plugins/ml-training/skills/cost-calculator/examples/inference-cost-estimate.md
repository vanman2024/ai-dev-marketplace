# Inference Cost Estimate Example

## Scenario: Production Inference for RedAI Trade Classifier

### Project Details

**Model:**
- Fine-tuned Llama 2 7B classifier
- Task: Classify job descriptions into 10 trade categories
- Deployment: Modal serverless endpoint
- Model size: 14GB (PEFT adapter + base model)

**Expected Traffic:**
- Initial launch: 1,000 requests/day
- Growth target (6 months): 10,000 requests/day
- Peak traffic: 2x average (marketing campaigns)

**Performance:**
- Average latency: 2 seconds per request
- Batch size: 1 (real-time classification)
- GPU: T4 (sufficient for 7B model)

---

## Cost Calculation: Current Traffic (1K/day)

### Modal Serverless (Recommended)

```bash
bash scripts/estimate-inference-cost.sh \
  --requests-per-day 1000 \
  --avg-latency 2 \
  --gpu t4 \
  --platform modal \
  --deployment serverless
```

**Results:**
```json
{
  "requests_per_day": 1000,
  "requests_per_month": 30000,
  "avg_latency_sec": 2,
  "gpu": "T4",
  "platform": "Modal",
  "deployment": "serverless",
  "cost_breakdown": {
    "daily_compute_seconds": 2000,
    "daily_cost": 0.33,
    "monthly_cost": 9.90,
    "cost_per_request": 0.00033
  },
  "scaling_analysis": {
    "requests_10k_day": 99.00,
    "requests_100k_day": 990.00
  },
  "dedicated_alternative": {
    "monthly_cost": 442.50,
    "break_even_requests_day": 4500
  }
}
```

**Analysis:**
- Daily Cost: $0.33
- Monthly Cost: $9.90
- Cost per Request: $0.00033 ($0.33 per 1,000 requests)
- GPU Utilization: 2,000 seconds/day = 0.6 hours/day (2.5% utilization)
- **Serverless is optimal** âœ… (dedicated would waste 97.5% of capacity)

---

### Dedicated Lambda A10 (Alternative)

```bash
bash scripts/estimate-inference-cost.sh \
  --requests-per-day 1000 \
  --avg-latency 2 \
  --gpu a10 \
  --platform lambda \
  --deployment dedicated
```

**Results:**
```json
{
  "deployment": "dedicated",
  "monthly_cost": 223.20,
  "cost_per_request": 0.00744
}
```

**Analysis:**
- Monthly Cost: $223.20 (24/7 instance at $0.31/hr)
- Cost per Request: $0.00744
- **Not recommended** - 22x more expensive than serverless for low traffic
- Break-even point: 4,500 requests/day (15% GPU utilization)

---

## Cost Calculation: Growth Phase (10K/day)

### Modal Serverless (Still Recommended)

```bash
bash scripts/estimate-inference-cost.sh \
  --requests-per-day 10000 \
  --avg-latency 2 \
  --gpu t4 \
  --platform modal \
  --deployment serverless
```

**Results:**
```json
{
  "requests_per_day": 10000,
  "monthly_cost": 99.00,
  "cost_per_request": 0.00033,
  "dedicated_alternative": {
    "monthly_cost": 442.50,
    "break_even_requests_day": 4500
  }
}
```

**Analysis:**
- Monthly Cost: $99.00
- Cost per Request: $0.00033 (same as before)
- GPU Utilization: 5.6 hours/day (23% utilization)
- **Still cheaper than dedicated** (break-even at 4,500 requests/day reached)
- Consider batch inference for further optimization

---

## Cost Calculation: High Traffic (100K/day)

### Modal Serverless

```bash
bash scripts/estimate-inference-cost.sh \
  --requests-per-day 100000 \
  --avg-latency 2 \
  --gpu t4 \
  --platform modal \
  --deployment serverless
```

**Results:**
- Monthly Cost: $990.00
- GPU Utilization: 55.6 hours/day (232% - needs 3 GPUs at peak)

### Dedicated Lambda A10 (Now Cheaper)

```bash
bash scripts/estimate-inference-cost.sh \
  --requests-per-day 100000 \
  --avg-latency 2 \
  --gpu a10 \
  --platform lambda \
  --deployment dedicated
```

**Results:**
- Monthly Cost: $223.20 (single 24/7 instance)
- **Recommendation:** Switch to dedicated at this scale âœ…
- Savings: $766.80/month (77% cheaper)

---

## Optimization: Batch Inference

### Current: Real-time Single Request

- Latency: 2 seconds per request
- Throughput: 0.5 requests/second

### With Batching: Process 10 Requests Together

```bash
bash scripts/estimate-inference-cost.sh \
  --requests-per-day 1000 \
  --avg-latency 0.23 \
  --gpu t4 \
  --platform modal \
  --deployment serverless \
  --batch-inference yes
```

**How batching works:**
- Collect 10 requests
- Process together: 2.3 seconds total (15% overhead)
- Per-request latency: 0.23 seconds average
- Trade-off: 10-30 second wait time to form batch

**Results:**
```json
{
  "requests_per_day": 1000,
  "avg_latency_sec": 0.23,
  "batch_inference": "yes",
  "monthly_cost": 1.14,
  "cost_per_request": 0.000038
}
```

**Analysis:**
- Monthly Cost: $1.14 (vs $9.90 without batching)
- **Savings: $8.76/month (88% cheaper)** âœ…
- Cost per Request: $0.000038 (vs $0.00033)
- Trade-off: Adds 10-30 second latency for batch formation

**When to use batching:**
- Non-realtime applications (email processing, background jobs)
- Batch reporting (nightly classification runs)
- Cost-sensitive workloads
- Acceptable to wait 10-30 seconds for results

**When NOT to use batching:**
- Real-time user-facing applications
- Interactive chatbots
- Low-latency requirements (<1 second)

---

## Platform Comparison: 1K Requests/Day

### Summary Table

| Platform | Deployment | GPU | Monthly Cost | Cost/Request | Break-even |
|----------|------------|-----|--------------|--------------|------------|
| **Modal** | Serverless | T4 | **$9.90** | $0.00033 | Always wins <4.5K/day â­ |
| Modal | Serverless + Batch | T4 | **$1.14** | $0.000038 | Best for non-realtime ðŸŽ‰ |
| Lambda | Dedicated | A10 | $223.20 | $0.00744 | Only at >50K/day |
| RunPod | Serverless | T4 | $10.08 | $0.00034 | Similar to Modal |

### Recommendations by Traffic Level

| Traffic/Day | Best Option | Monthly Cost | Notes |
|-------------|-------------|--------------|-------|
| <1K | Modal Serverless | $5-10 | Stay serverless |
| 1K-10K | Modal Serverless | $10-100 | Still cost-effective |
| 10K-50K | Modal Serverless or Batch | $100-500 | Consider batching |
| 50K-100K | Dedicated Lambda A10 | $223 | Dedicated now cheaper |
| >100K | Multi-GPU Dedicated | $450+ | Scale horizontally |

---

## Real-World Example: RedAI Inference

### Phase 1: Launch (Months 1-3)
- Traffic: 500-1,000 requests/day
- Platform: Modal serverless T4
- Monthly cost: $5-10
- Features: Auto-scaling, no idle cost

### Phase 2: Growth (Months 4-6)
- Traffic: 2,000-5,000 requests/day
- Platform: Modal serverless T4
- Monthly cost: $20-50
- Features: Handles traffic spikes automatically

### Phase 3: Scale (Months 7-12)
- Traffic: 10,000-50,000 requests/day
- Platform: Modal serverless with batch inference
- Monthly cost: $50-200 (with batching)
- Features: Background job processing for non-critical requests

### Phase 4: Enterprise (Year 2+)
- Traffic: >100,000 requests/day
- Platform: Dedicated Lambda A10 cluster
- Monthly cost: $223-450 (1-2 instances)
- Features: Predictable costs, lower latency

---

## Cost Breakdown: 6-Month Projection

### Conservative Growth (500 â†’ 5,000 requests/day)

| Month | Avg Requests/Day | Platform | Monthly Cost | Cumulative |
|-------|------------------|----------|--------------|------------|
| 1 | 500 | Modal Serverless | $5.00 | $5.00 |
| 2 | 1,000 | Modal Serverless | $9.90 | $14.90 |
| 3 | 1,500 | Modal Serverless | $14.85 | $29.75 |
| 4 | 2,500 | Modal Serverless | $24.75 | $54.50 |
| 5 | 4,000 | Modal Serverless | $39.60 | $94.10 |
| 6 | 5,000 | Modal Serverless | $49.50 | $143.60 |

**6-Month Total: $143.60**

### With Batch Optimization (Month 4+)

| Month | Requests/Day | Batch% | Monthly Cost | Savings |
|-------|--------------|--------|--------------|---------|
| 4 | 2,500 | 50% | $15.00 | $9.75 |
| 5 | 4,000 | 60% | $18.50 | $21.10 |
| 6 | 5,000 | 70% | $19.80 | $29.70 |

**6-Month Total with Batching: $88.55**
**Total Savings: $55.05 (38% cheaper)** âœ…

---

## Combined Training + Inference Budget

### Monthly Costs (Month 6)

| Component | Details | Monthly Cost |
|-----------|---------|--------------|
| Training | 1 run/month (Lambda A10) | $0.62 |
| Inference | 5,000 req/day (Modal serverless) | $49.50 |
| Storage | 14GB model + 5GB data | $0.44 |
| **Total** | | **$50.56** |

### With Optimizations

| Component | Optimization | Cost | Savings |
|-----------|--------------|------|---------|
| Training | PEFT + FP16 | $0.62 | $7.78 (vs full fine-tune) |
| Inference | 70% batching | $19.80 | $29.70 |
| Storage | Compression | $0.30 | $0.14 |
| **Total** | | **$20.72** | **$37.62 (64%)** |

**Total Monthly Cost: $20.72**
**vs Local GPU (RTX 4090): $300/month (power + amortization)**
**Savings: $279.28/month (93% cheaper)** ðŸŽ‰

---

## Scaling Cost Projections

### Break-even Analysis

**When does dedicated become cheaper than serverless?**

```bash
bash scripts/estimate-inference-cost.sh \
  --requests-per-day 4500 \
  --avg-latency 2 \
  --gpu t4 \
  --platform modal \
  --deployment serverless
```

**Results:**
- Serverless cost at 4,500/day: $223/month
- Dedicated Lambda A10 cost: $223/month
- **Break-even point: 4,500 requests/day** (15% GPU utilization)

### Traffic Thresholds

| Daily Requests | Monthly Cost (Serverless) | Monthly Cost (Dedicated) | Recommendation |
|----------------|---------------------------|--------------------------|----------------|
| 500 | $5 | $223 | Serverless (45x cheaper) |
| 1,000 | $10 | $223 | Serverless (22x cheaper) |
| 2,500 | $25 | $223 | Serverless (9x cheaper) |
| 4,500 | $223 | $223 | Either (break-even) |
| 10,000 | $495 | $223 | Dedicated (2.2x cheaper) |
| 50,000 | $2,475 | $223 | Dedicated (11x cheaper) |

---

## Key Takeaways

1. **Serverless optimal for <4.5K requests/day**: Modal T4 wins
2. **Batch inference saves 88%**: Use for background jobs
3. **Dedicated cheaper at >50K/day**: Switch to Lambda A10
4. **Cloud 93% cheaper than local GPU**: No upfront cost
5. **Start small, scale seamlessly**: Modal auto-scales 0-1000 GPUs
6. **Monitor actual usage**: Adjust based on real traffic patterns

---

**Next Steps:**
1. Deploy model to Modal serverless
2. Monitor actual request volume and latency
3. Switch to batch inference for background jobs (88% savings)
4. Plan dedicated migration at 50K requests/day
5. Apply for Modal credits ($50K free for startups)

---

**Related:**
- See `training-cost-estimate.md` for training costs
- See `../templates/cost-breakdown.json` for full budget template
- Use `scripts/compare-platforms.sh` to compare alternatives
