# Text Generation Example

Complete example of building a question-answering text generator using T5.

## Overview

**Task:** Generate answers to questions based on context
**Approach:** Seq2Seq fine-tuning with T5
**Dataset:** 5,000 question-answer pairs
**Training time:** ~45 minutes on RTX 3090

## Setup

```bash
# Create project
cd /path/to/ml-training/skills/training-patterns
./scripts/setup-generation.sh qa-generator t5-small question-answering

cd qa-generator
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Dataset Preparation

### Format

JSON with question-answer pairs:

```json
[
  {
    "question": "What is machine learning?",
    "answer": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed."
  },
  {
    "question": "How does gradient descent work?",
    "answer": "Gradient descent is an optimization algorithm that iteratively adjusts model parameters in the direction that minimizes the loss function."
  }
]
```

### Dataset Split

- **Training:** 4,000 QA pairs (80%)
- **Validation:** 500 QA pairs (10%)
- **Test:** 500 QA pairs (10%)

Place files in:
- `data/train.json`
- `data/val.json`
- `data/test.json`

### Example Dataset Creation

```python
import json

qa_pairs = [
    {
        "question": "What is Python?",
        "answer": "Python is a high-level, interpreted programming language known for its simplicity and readability."
    },
    {
        "question": "What is a neural network?",
        "answer": "A neural network is a computational model inspired by biological neural networks that processes information through interconnected nodes."
    },
    # Add more pairs...
]

# Save
with open('data/train.json', 'w') as f:
    json.dump(qa_pairs, f, indent=2)
```

## Training Configuration

Edit `config.yaml`:

```yaml
model:
  name: t5-small
  task_type: generation
  generation_type: question-answering

dataset:
  train_file: data/train.json
  validation_file: data/val.json
  input_column: question
  target_column: answer
  max_input_length: 512
  max_target_length: 128

training:
  num_epochs: 5
  batch_size: 8
  learning_rate: 3e-4
  warmup_steps: 1000
  gradient_accumulation_steps: 2
  fp16: true

generation:
  max_length: 128
  num_beams: 4
  length_penalty: 2.0
  early_stopping: true
```

## Training

```bash
python train.py --config config.yaml
```

### Training Output

```
Using device: cuda
ðŸŽ® GPU: NVIDIA GeForce RTX 3090

ðŸ“¥ Loading tokenizer: t5-small
ðŸ“š Loading datasets...
âœ… Train: 4000
âœ… Validation: 500

Sample data point:
  question: What is machine learning?
  answer: Machine learning is a subset of artificial intelligence...

Tokenizing datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000

ðŸ“¥ Loading model: t5-small

===========================================================
ðŸš€ Starting training...
===========================================================

Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:12<00:00]
Evaluation: {'rouge1': 0.432, 'rouge2': 0.218, 'rougeL': 0.389}

Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:08<00:00]
Evaluation: {'rouge1': 0.521, 'rouge2': 0.312, 'rougeL': 0.478}

Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:11<00:00]
Evaluation: {'rouge1': 0.587, 'rouge2': 0.389, 'rougeL': 0.542}

Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:09<00:00]
Evaluation: {'rouge1': 0.623, 'rouge2': 0.428, 'rougeL': 0.581}

Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 250/250 [09:10<00:00]
Evaluation: {'rouge1': 0.641, 'rouge2': 0.452, 'rougeL': 0.598}

ðŸ’¾ Saving model...

===========================================================
âœ… Training completed!
===========================================================
```

## Inference

### Single Question

```python
from generate import TextGenerator

generator = TextGenerator('./final_model')

question = "What is deep learning?"
answer = generator.generate(question, max_length=128, num_beams=4)

print(f"Q: {question}")
print(f"A: {answer}")
```

**Output:**
```
Q: What is deep learning?
A: Deep learning is a subset of machine learning that uses multi-layered neural networks to learn hierarchical representations of data, enabling the model to automatically discover intricate patterns and features.
```

### Batch Generation

```python
questions = [
    "What is natural language processing?",
    "How does a transformer model work?",
    "What is the attention mechanism?"
]

answers = generator.generate_batch(
    questions,
    max_length=128,
    num_beams=4,
    batch_size=8
)

for q, a in zip(questions, answers):
    print(f"Q: {q}")
    print(f"A: {a}\n")
```

### Advanced Generation Parameters

```python
# More creative (higher temperature)
answer = generator.generate(
    question,
    max_length=128,
    num_beams=4,
    temperature=0.9,
    top_p=0.95,
    do_sample=True
)

# More focused (beam search)
answer = generator.generate(
    question,
    max_length=128,
    num_beams=8,
    length_penalty=2.0,
    early_stopping=True
)

# Longer answers
answer = generator.generate(
    question,
    max_length=256,
    min_length=50,
    num_beams=4
)
```

## Results

### ROUGE Scores

| Metric | Validation | Test |
|--------|-----------|------|
| ROUGE-1 | 64.1% | 63.8% |
| ROUGE-2 | 45.2% | 44.6% |
| ROUGE-L | 59.8% | 59.3% |

**Interpretation:**
- **ROUGE-1:** 64% unigram overlap (word-level match)
- **ROUGE-2:** 45% bigram overlap (2-word phrases)
- **ROUGE-L:** 60% longest common subsequence

### Example Predictions

**Question:** "What is supervised learning?"

**Generated Answer:**
"Supervised learning is a machine learning approach where the model is trained on labeled data, learning to map inputs to outputs based on example input-output pairs provided during training."

**Ground Truth:**
"Supervised learning involves training models on labeled datasets where each input has a corresponding target output, allowing the model to learn the relationship between inputs and outputs."

**ROUGE-L:** 0.73 (good match!)

---

**Question:** "Explain backpropagation."

**Generated Answer:**
"Backpropagation is an algorithm used to train neural networks by calculating gradients of the loss function with respect to each weight by applying the chain rule, propagating errors backward through the network."

**Ground Truth:**
"Backpropagation computes gradients by propagating error signals backward from output to input layers, using the chain rule to calculate how much each parameter contributed to the error."

**ROUGE-L:** 0.68 (decent match)

## Generation Quality Analysis

### Strengths

âœ… **Factually accurate** answers
âœ… **Coherent and fluent** text
âœ… **Appropriate length** (not too short/long)
âœ… **Technical terminology** used correctly
âœ… **Grammar and syntax** are correct

### Weaknesses

âŒ **Occasional hallucinations** (makes up facts)
âŒ **Repetitive phrases** sometimes
âŒ **Lacks depth** for complex topics
âŒ **Generic responses** for ambiguous questions

### Improvement Strategies

1. **Larger model:** Use `t5-base` or `t5-large`
2. **More data:** Increase training examples to 50k+
3. **Better prompts:** Add "explain in detail" to questions
4. **Post-processing:** Filter repetitive n-grams
5. **Retrieval augmentation:** Add context from knowledge base

## Resource Usage

- **Training time:** 46 minutes (5 epochs)
- **GPU memory:** 10.3 GB (peak)
- **Model size:** 242 MB (t5-small)
- **Inference speed:** ~45ms per question (GPU)
- **Throughput:** ~220 questions/second (batch 32)

## Model Variants

### T5-Small (Default)

- **Parameters:** 60M
- **Memory:** ~10GB training
- **Speed:** Fast inference
- **Quality:** Good for prototypes

### T5-Base

- **Parameters:** 220M
- **Memory:** ~16GB training
- **Speed:** Medium inference
- **Quality:** Better answers

```bash
./scripts/setup-generation.sh qa-generator-base t5-base question-answering
```

### T5-Large

- **Parameters:** 770M
- **Memory:** ~24GB+ training
- **Speed:** Slower inference
- **Quality:** Best answers

```bash
./scripts/setup-generation.sh qa-generator-large t5-large question-answering
```

## Hyperparameter Tuning

### Learning Rate Experiments

| LR | ROUGE-L | Notes |
|----|---------|-------|
| 1e-4 | 54.2% | Too slow |
| 3e-4 | **59.8%** | Best |
| 5e-4 | 57.3% | Unstable |
| 1e-3 | 51.6% | Too high |

### Beam Size Impact

| Beams | ROUGE-L | Time/Q |
|-------|---------|--------|
| 1 | 52.1% | 18ms |
| 2 | 56.8% | 28ms |
| **4** | **59.8%** | **45ms** |
| 8 | 60.2% | 89ms |

**Recommendation:** num_beams=4 (best quality/speed)

## Production Deployment

### API Endpoint

```python
from fastapi import FastAPI
from generate import TextGenerator

app = FastAPI()
generator = TextGenerator('./final_model')

@app.post("/answer")
async def answer_question(question: str):
    answer = generator.generate(
        question,
        max_length=128,
        num_beams=4
    )
    return {"question": question, "answer": answer}
```

### Batch Processing

```python
import pandas as pd

# Load questions
questions_df = pd.read_csv('questions.csv')

# Generate answers in batches
batch_size = 32
answers = []

for i in range(0, len(questions_df), batch_size):
    batch = questions_df['question'][i:i+batch_size].tolist()
    batch_answers = generator.generate_batch(batch, batch_size=batch_size)
    answers.extend(batch_answers)

# Save results
questions_df['answer'] = answers
questions_df.to_csv('qa_results.csv', index=False)
```

## Advanced Techniques

### Prompt Engineering

```python
# Add context to questions
def enhanced_question(question, context=""):
    if context:
        return f"Context: {context}\n\nQuestion: {question}\n\nAnswer:"
    return f"Question: {question}\n\nAnswer:"

question = "What is Python?"
context = "We are discussing programming languages used in data science."

answer = generator.generate(enhanced_question(question, context))
```

### Constrained Generation

```python
# Force answer to start with specific words
prefix = "Python is"
answer = generator.generate(
    question,
    prefix=prefix,
    max_length=128
)
```

## Key Takeaways

âœ… **T5 achieved 60% ROUGE-L** for QA generation
âœ… **Fast training** (46 minutes for 5 epochs)
âœ… **Good inference speed** (220 questions/second)
âœ… **Production-ready** with minimal setup
âœ… **Scalable** (can upgrade to t5-base/large)

## Next Steps

1. **Scale up:** Try `t5-base` or `flan-t5-base`
2. **More data:** Collect 50k+ QA pairs
3. **Domain-specific:** Fine-tune on your domain
4. **RAG integration:** Add retrieval for factual grounding
5. **Evaluation:** Human evaluation for quality

---

**Generated by ML Training Plugin**
**Training pattern: Seq2Seq Generation**
